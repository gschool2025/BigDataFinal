{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T15:38:31.294055Z",
     "start_time": "2026-01-05T15:38:31.289169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip install selenium\n",
    "# !pip install pymongo\n",
    "# !pip install numpy\n",
    "# !pip install BeautifulSoup\n",
    "# !pip install python-dotenv pymongo dnspython\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T15:39:26.491779Z",
     "start_time": "2026-01-05T15:38:31.314070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Please check README.md first\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class WebScraping:\n",
    "    def __init__(self, query_term='business-analyst'):\n",
    "        print('Initialise WebScraping instance')\n",
    "        self.query_term = query_term\n",
    "        self.target_url = f\"https://nofluffjobs.com/pl/?lang=en&criteria=jobPosition%3D{self.query_term}\"\n",
    "        self.final_html = ''\n",
    "        # --- init db: local db / Atlas Mongodb ---\n",
    "        self._init_db()\n",
    "\n",
    "\n",
    "    def _init_db(self):\n",
    "        \"\"\"\n",
    "        You don't need .env, cause you will deploy on local db\n",
    "        MONGO_MODE from .env:\n",
    "        \"local\": connect with local mongodb\n",
    "        \"atlas\": connect with remote mongodb\n",
    "        \"\"\"\n",
    "\n",
    "        mode = os.getenv(\"MONGO_MODE\", \"local\")\n",
    "        db_name = os.getenv(\"DB_NAME\", \"BD_final\")\n",
    "\n",
    "        print(\"DB MODE: \", mode)\n",
    "\n",
    "        if mode == \"atlas\":\n",
    "            uri = os.getenv(\"ATLAS_MONGO_URI\")\n",
    "            if not uri:\n",
    "                raise ValueError(\"‚ùå Error: ATLAS_MONGO_URI not found in .env while mode is 'atlas'\")\n",
    "            print(f\"üåê Connecting to Remote MongoDB Atlas...\")\n",
    "        else:\n",
    "            uri = \"mongodb://localhost:27017/\"\n",
    "            print(f\" Connecting to Local MongoDB ({uri})...\")\n",
    "\n",
    "        try:\n",
    "            self.client = MongoClient(uri)\n",
    "            # ÊµãËØïËøûÊé•\n",
    "            self.client.admin.command('ping')\n",
    "            self.db = self.client[db_name]\n",
    "            print(f\"‚úÖ Successfully connected to database: {db_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Database connection failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def scrape_save_raw_to_db(self, clicks=10):\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.maximize_window()\n",
    "        driver.get(self.target_url)\n",
    "        # Give you 15 seconds to accept/cancel all popup window\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "\n",
    "        # # A. Handle Cookie pop-ups\n",
    "        # try:\n",
    "        #     cookie_btn = wait.until(EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\")))\n",
    "        #     cookie_btn.click()\n",
    "        #     print(\"Cookies accepted.\")\n",
    "        # except:\n",
    "        #     print(\"No cookie banner found or already accepted.\")\n",
    "\n",
    "        # B. Loop to click \"See more offers\" button 10 times\n",
    "        count = 0\n",
    "        while count < clicks:\n",
    "            try:\n",
    "                # Find the button with the nfjloadmore attribute\n",
    "                load_more_btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[nfjloadmore]\")))\n",
    "\n",
    "                # Scroll to the button position to ensure it is in the viewport\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more_btn)\n",
    "                time.sleep(1.5) # Allow some buffer time for scrolling\n",
    "\n",
    "                # Click to load more\n",
    "                if load_more_btn:\n",
    "                    load_more_btn.click()\n",
    "                count += 1\n",
    "                print(f\"Clicked 'See more' ({count}/{clicks})\")\n",
    "\n",
    "                # Delay for new content to load\n",
    "                time.sleep(2.5)\n",
    "            except Exception as e:\n",
    "                print(f\"Finished loading or button not found: {e}\")\n",
    "                break\n",
    "\n",
    "        # C. Get the complete HTML after 10 clicks and save it\n",
    "        print(\"All pages loaded. Capturing final HTML...\")\n",
    "        self.final_html = driver.page_source\n",
    "        self.save_raw_to_mongodb()\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "    def save_raw_to_mongodb(self):\n",
    "        \"\"\"\n",
    "        Reference your initial logic to store the raw HTML into NoSQL\n",
    "        \"\"\"\n",
    "        # only hold one raw_json html data\n",
    "        self.db.jobs_raw.drop()\n",
    "        website_document = {\n",
    "            'url': self.target_url,\n",
    "            'content': self.final_html,\n",
    "            'date': datetime.now(),\n",
    "            'query_term': self.query_term\n",
    "        }\n",
    "\n",
    "        # Store in the jobs_raw collection\n",
    "        result = self.db.jobs_raw.insert_one(website_document)\n",
    "        print(f\"Raw HTML saved to MongoDB! Document ID: {result.inserted_id}\")\n",
    "\n",
    "    def _parse_salary(self, salary_str):\n",
    "        \"\"\"\n",
    "        Internal helper method: Parse salary string\n",
    "        Example input: \"10 000 - 15 000 PLN\", \"20 000 PLN\", \"Undisclosed\"\n",
    "        Output: (min_salary, max_salary)\n",
    "        \"\"\"\n",
    "        if not salary_str or \"Undisclosed\" in salary_str or \"Agreement\" in salary_str:\n",
    "            return None, None\n",
    "\n",
    "        # Remove spaces and thousands separators\n",
    "        clean_str = salary_str.replace('\\xa0', '').replace(' ', '')\n",
    "\n",
    "        # Match all digits\n",
    "        numbers = re.findall(r'\\d+', clean_str)\n",
    "\n",
    "        try:\n",
    "            if len(numbers) >= 2:\n",
    "                # Range salary: [10000, 15000]\n",
    "                return float(numbers[0]), float(numbers[1])\n",
    "            elif len(numbers) == 1:\n",
    "                # Fixed salary: [20000]\n",
    "                return float(numbers[0]), float(numbers[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return None, None\n",
    "\n",
    "    def _parse_location(self, card):\n",
    "        \"\"\"\n",
    "        Internal helper method: Parse location\n",
    "        \"\"\"\n",
    "        location_tag = card.select_one('span.posting-info__location, nfj-posting-item-city')\n",
    "        if not location_tag:\n",
    "            return \"Unknown\"\n",
    "\n",
    "        loc_text = location_tag.get_text(strip=True)\n",
    "\n",
    "        # Special case handling: Remote work\n",
    "        if \"Remote\" in loc_text or \"Zdalna\" in loc_text:\n",
    "            return \"Remote\"\n",
    "\n",
    "        # Extract city name (some include '+1', filter via split)\n",
    "        city = loc_text.split('+')[0].strip()\n",
    "        return city\n",
    "\n",
    "    def process_and_save(self):\n",
    "        \"\"\"\n",
    "        Read HTML from jobs_raw and extract fields to store in jobs_processed\n",
    "        extract raw html and parse fields then save into\n",
    "        \"\"\"\n",
    "        # 1. Get the most recently scraped HTML document\n",
    "        raw_data = self.db.jobs_raw.find_one(sort=[(\"date\", -1)])\n",
    "        if not raw_data:\n",
    "            print(\"No raw data found in MongoDB!\")\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(raw_data['content'], 'html.parser')\n",
    "\n",
    "        # 2. Locate all job cards\n",
    "        postings = soup.select('a.posting-list-item')\n",
    "        print(f\"Found {len(postings)} job postings in HTML.\")\n",
    "\n",
    "        processed_list = []\n",
    "        base_domain = \"https://nofluffjobs.com\" # Used to concatenate the full URL\n",
    "\n",
    "        for post in postings:\n",
    "            try:\n",
    "                # --- Field 1: Job Title ---\n",
    "                title_el = post.select_one('h3.posting-title__position, .posting-title__can-hide')\n",
    "                if title_el:\n",
    "                    # Find and remove any potential \"NEW\" tags or other badges\n",
    "                    # NFJ's badge class names usually contain title-badge\n",
    "                    for badge in title_el.select('.title-badge, .title-badge--new'):\n",
    "                        badge.decompose()\n",
    "                    job_title = title_el.get_text(strip=True)\n",
    "                else:\n",
    "                    job_title = \"N/A\"\n",
    "                # print('JOB TITLE:', job_title)\n",
    "\n",
    "                # --- Field 2: Company Name ---\n",
    "                company_el = post.select_one('span.d-block, .company-name')\n",
    "                company_name = company_el.get_text(strip=True) if company_el else \"N/A\"\n",
    "\n",
    "                # --- Fields 3 & 4: Min/Max Salary ---\n",
    "                salary_el = post.select_one('span.text-truncate, nfj-posting-item-salary')\n",
    "                salary_str = salary_el.get_text(strip=True) if salary_el else \"\"\n",
    "                min_sal, max_sal = self._parse_salary(salary_str)\n",
    "\n",
    "                # --- Field 5: Location ---\n",
    "                location = self._parse_location(post)\n",
    "\n",
    "                # --- Field 6: Jump URL (New) ---\n",
    "                # Get relative path from the <a> tag's href attribute and concatenate domain\n",
    "                relative_url = post.get('href')\n",
    "                jump_url = base_domain + relative_url if relative_url else \"N/A\"\n",
    "\n",
    "                # Construct document\n",
    "                job_doc = {\n",
    "                    'job_title': job_title,\n",
    "                    'company_name': company_name,\n",
    "                    'min_salary': min_sal,\n",
    "                    'max_salary': max_sal,\n",
    "                    'location': location,\n",
    "                    'jump_url': jump_url, # Add to the document\n",
    "                    'processed_at': datetime.now(),\n",
    "                    'query_term':raw_data['query_term']\n",
    "                }\n",
    "\n",
    "                processed_list.append(job_doc)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing a single post: {e}\")\n",
    "                continue\n",
    "\n",
    "        # 3. Batch save to the new collection\n",
    "        if processed_list:\n",
    "            self.db.jobs_processed.drop()\n",
    "            self.db.jobs_processed.insert_many(processed_list)\n",
    "            print(f\"Successfully processed {len(processed_list)} jobs and saved to 'jobs_processed'.\")\n",
    "\n",
    "\n",
    "# run the WebScrapper\n",
    "# query and save the raw html data into mongodb\n",
    "scraper = WebScraping(query_term='business-analyst')\n",
    "scraper.scrape_save_raw_to_db()\n",
    "# extract data, and parse to fields and save into db again\n",
    "scraper.process_and_save()"
   ],
   "id": "6eb5150b233b2a92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialise WebScraping instance\n",
      "DB MODE:  atlas\n",
      "üåê Connecting to Remote MongoDB Atlas...\n",
      "‚úÖ Successfully connected to database: BD_final\n",
      "Clicked 'See more' (1/10)\n",
      "Clicked 'See more' (2/10)\n",
      "Clicked 'See more' (3/10)\n",
      "Clicked 'See more' (4/10)\n",
      "Clicked 'See more' (5/10)\n",
      "Clicked 'See more' (6/10)\n",
      "Finished loading or button not found: Message: \n",
      "Stacktrace:\n",
      "Symbols not available. Dumping unresolved backtrace:\n",
      "\t0x7ff69e7088e5\n",
      "\t0x7ff69e708940\n",
      "\t0x7ff69e4e165d\n",
      "\t0x7ff69e539a33\n",
      "\t0x7ff69e539d3c\n",
      "\t0x7ff69e58df67\n",
      "\t0x7ff69e58ac97\n",
      "\t0x7ff69e52ac29\n",
      "\t0x7ff69e52ba93\n",
      "\t0x7ff69ea20640\n",
      "\t0x7ff69ea1af80\n",
      "\t0x7ff69ea396e6\n",
      "\t0x7ff69e725de4\n",
      "\t0x7ff69e72ed8c\n",
      "\t0x7ff69e712004\n",
      "\t0x7ff69e7121b5\n",
      "\t0x7ff69e6f7ee2\n",
      "\t0x7ff8c6e2e8d7\n",
      "\t0x7ff8c7a4c53c\n",
      "\n",
      "All pages loaded. Capturing final HTML...\n",
      "Raw HTML saved to MongoDB! Document ID: 695bdb245773267e73f5a7d7\n",
      "Found 123 job postings in HTML.\n",
      "Successfully processed 123 jobs and saved to 'jobs_processed'.\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
