{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbc121e30a2defb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium\n",
    "# !pip install pymongo\n",
    "# !pip install numpy\n",
    "# !pip install BeautifulSoup\n",
    "# !pip install python-dotenv pymongo dnspython\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eb5150b233b2a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please check README.md first\n",
    "\n",
    "'''\n",
    "URL='https://nofluffjobs.com/api/search/posting?withSalaryMatch=true&pageTo=2&pageSize=20&salaryCurrency=PLN&salaryPeriod=month&region=pl&language=en-GB'\n",
    "If we use request.post(urlÔºåpayload)\n",
    "Is's easier to get more details from one query\n",
    "'''\n",
    "\n",
    "# load environment variable to init db\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class WebScraping:\n",
    "    def __init__(self, query_term='backend'):\n",
    "        print('Initialise WebScraping instance')\n",
    "        self.query_term = query_term\n",
    "        self.target_url = f\"https://nofluffjobs.com/pl/?lang=en&criteria=jobPosition%3D{self.query_term}\"\n",
    "        self.final_html = ''\n",
    "        # --- init db: local db / Atlas Mongodb ---\n",
    "        self._init_db()\n",
    "\n",
    "\n",
    "    def _init_db(self):\n",
    "        \"\"\"\n",
    "        You don't need .env, cause you will deploy on local db\n",
    "        MONGO_MODE from .env:\n",
    "        \"local\": connect with local mongodb\n",
    "        \"atlas\": connect with remote mongodb\n",
    "        \"\"\"\n",
    "\n",
    "        mode = os.getenv(\"MONGO_MODE\", \"local\")\n",
    "        db_name = os.getenv(\"DB_NAME\", \"BD_final\")\n",
    "\n",
    "        print(\"DB MODE: \", mode)\n",
    "\n",
    "        if mode == \"atlas\":\n",
    "            uri = os.getenv(\"ATLAS_MONGO_URI\")\n",
    "            if not uri:\n",
    "                raise ValueError(\"‚ùå Error: ATLAS_MONGO_URI not found in .env while mode is 'atlas'\")\n",
    "            print(f\"üåê Connecting to Remote MongoDB Atlas...\")\n",
    "        else:\n",
    "            uri = \"mongodb://localhost:27017/\"\n",
    "            print(f\" Connecting to Local MongoDB ({uri})...\")\n",
    "\n",
    "        try:\n",
    "            self.client = MongoClient(uri)\n",
    "            # ÊµãËØïËøûÊé•\n",
    "            self.client.admin.command('ping')\n",
    "            self.db = self.client[db_name]\n",
    "            print(f\"‚úÖ Successfully connected to database: {db_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Database connection failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def scrape_save_raw_to_db(self, clicks=50):\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(self.target_url)\n",
    "        driver.maximize_window()\n",
    "        # Give you 15 seconds to accept/cancel all popup window\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "\n",
    "        # # A. Handle Cookie pop-ups\n",
    "        # try:\n",
    "        #     cookie_btn = wait.until(EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\")))\n",
    "        #     cookie_btn.click()\n",
    "        #     print(\"Cookies accepted.\")\n",
    "        # except:\n",
    "        #     print(\"No cookie banner found or already accepted.\")\n",
    "\n",
    "        # B. Loop to click \"See more offers\" button 10 times\n",
    "        count = 0\n",
    "        while count < clicks:\n",
    "            try:\n",
    "                # Find the button with the nfjloadmore attribute\n",
    "                load_more_btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[nfjloadmore]\")))\n",
    "\n",
    "                # Scroll to the button position to ensure it is in the viewport\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more_btn)\n",
    "                time.sleep(1.5) # Allow some buffer time for scrolling\n",
    "\n",
    "                # Click to load more\n",
    "                if load_more_btn:\n",
    "                    load_more_btn.click()\n",
    "                count += 1\n",
    "                print(f\"Clicked 'See more' ({count}/{clicks})\")\n",
    "\n",
    "                # Delay for new content to load\n",
    "                time.sleep(2.5)\n",
    "            except Exception as e:\n",
    "                print(f\"Finished loading or button not found: {e}\")\n",
    "                break\n",
    "\n",
    "        # C. Get the complete HTML after 10 clicks and save it\n",
    "        print(\"All pages loaded. Capturing final HTML...\")\n",
    "        self.final_html = driver.page_source\n",
    "        self.save_raw_to_mongodb()\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "    def save_raw_to_mongodb(self):\n",
    "        \"\"\"\n",
    "        Reference your initial logic to store the raw HTML into NoSQL\n",
    "        \"\"\"\n",
    "        # only hold one raw_json html data\n",
    "        self.db.jobs_raw.drop()\n",
    "        website_document = {\n",
    "            'url': self.target_url,\n",
    "            'content': self.final_html,\n",
    "            'date': datetime.now(),\n",
    "            'query_term': self.query_term\n",
    "        }\n",
    "\n",
    "        # Store in the jobs_raw collection\n",
    "        result = self.db.jobs_raw.insert_one(website_document)\n",
    "        print(f\"Raw HTML saved to MongoDB! Document ID: {result.inserted_id}\")\n",
    "\n",
    "    # parse and split salary to min and max two value\n",
    "    def parse_salary(self, salary_str):\n",
    "        \"\"\"\n",
    "        Internal helper method: Parse salary string\n",
    "        Example input: \"10 000 - 15 000 PLN\", \"20 000 PLN\", \"Undisclosed\"\n",
    "        Output: (min_salary, max_salary)\n",
    "        \"\"\"\n",
    "        if not salary_str or \"Undisclosed\" in salary_str or \"Agreement\" in salary_str:\n",
    "            return None, None\n",
    "\n",
    "        # Remove spaces and thousands separators\n",
    "        clean_str = salary_str.replace('\\xa0', '').replace(' ', '')\n",
    "\n",
    "        # Match all digits\n",
    "        numbers = re.findall(r'\\d+', clean_str)\n",
    "\n",
    "        try:\n",
    "            if len(numbers) >= 2:\n",
    "                # Range salary: [10000, 15000]\n",
    "                return float(numbers[0]), float(numbers[1])\n",
    "            elif len(numbers) == 1:\n",
    "                # Fixed salary: [20000]\n",
    "                return float(numbers[0]), float(numbers[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return None, None\n",
    "\n",
    "    # parse job location\n",
    "    def parse_location(self, card):\n",
    "        \"\"\"\n",
    "        Internal helper method: Parse location\n",
    "        \"\"\"\n",
    "        location_tag = card.select_one('span.posting-info__location, nfj-posting-item-city')\n",
    "        if not location_tag:\n",
    "            return \"Unknown\"\n",
    "\n",
    "        loc_text = location_tag.get_text(strip=True)\n",
    "\n",
    "        # Special case handling: Remote work\n",
    "        if \"Remote\" in loc_text or \"Zdalna\" in loc_text:\n",
    "            return \"Remote\"\n",
    "\n",
    "        # Extract city name (some include '+1', filter via split)\n",
    "        city = loc_text.split('+')[0].strip()\n",
    "        return city\n",
    "\n",
    "    # parse job title,company name, Min/Max Salary,Location, Jump URL and save into db\n",
    "    def process_and_save(self):\n",
    "        \"\"\"\n",
    "        Read HTML from jobs_raw and extract fields to store in jobs_processed\n",
    "        extract raw html and parse fields then save into\n",
    "        \"\"\"\n",
    "        # Get the most recently scraped HTML document\n",
    "        raw_data = self.db.jobs_raw.find_one(sort=[(\"date\", -1)])\n",
    "        if not raw_data:\n",
    "            print(\"No raw data found in MongoDB!\")\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(raw_data['content'], 'html.parser')\n",
    "\n",
    "        # Locate all job cards\n",
    "        postings = soup.select('a.posting-list-item')\n",
    "        print(f\"Found {len(postings)} job postings in HTML.\")\n",
    "\n",
    "        processed_list = []\n",
    "        base_domain = \"https://nofluffjobs.com\" # Used to concatenate the full URL\n",
    "\n",
    "        for post in postings:\n",
    "            try:\n",
    "                # --- Field 1: Job Title ---\n",
    "                title_el = post.select_one('h3.posting-title__position, .posting-title__can-hide')\n",
    "                if title_el:\n",
    "                    # Find and remove any potential \"NEW\" tags or other badges\n",
    "                    # NFJ's badge class names usually contain title-badge\n",
    "                    for badge in title_el.select('.title-badge, .title-badge--new'):\n",
    "                        badge.decompose()\n",
    "                    job_title = title_el.get_text(strip=True)\n",
    "                else:\n",
    "                    job_title = \"N/A\"\n",
    "                # print('JOB TITLE:', job_title)\n",
    "\n",
    "                # --- Field 2: Company Name ---\n",
    "                company_el = post.select_one('span.d-block, .company-name')\n",
    "                company_name = company_el.get_text(strip=True) if company_el else \"N/A\"\n",
    "\n",
    "                # --- Fields 3 & 4: Min/Max Salary ---\n",
    "                salary_el = post.select_one('span.text-truncate, nfj-posting-item-salary')\n",
    "                salary_str = salary_el.get_text(strip=True) if salary_el else \"\"\n",
    "                min_sal, max_sal = self.parse_salary(salary_str)\n",
    "\n",
    "                # --- Field 5: Location ---\n",
    "                location = self.parse_location(post)\n",
    "\n",
    "                # --- Field 6: Jump URL ---\n",
    "                # Get relative path from the <a> tag's href attribute and concatenate domain\n",
    "                relative_url = post.get('href')\n",
    "                jump_url = base_domain + relative_url if relative_url else \"N/A\"\n",
    "\n",
    "                # Construct document\n",
    "                job_doc = {\n",
    "                    \"source\": \"nofluffjobs\",\n",
    "                    'job_title': job_title,\n",
    "                    'company_name': company_name,\n",
    "                    'min_salary': min_sal,\n",
    "                    'max_salary': max_sal,\n",
    "                    'location': location,\n",
    "                    'jump_url': jump_url, # Add to the document\n",
    "                    'processed_at': datetime.now(),\n",
    "                    'query_term':raw_data['query_term']\n",
    "                }\n",
    "\n",
    "                processed_list.append(job_doc)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing a single post: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Batch save to the new collection\n",
    "        if processed_list:\n",
    "            self.db.jobs_processed.drop()\n",
    "            self.db.jobs_processed.insert_many(processed_list)\n",
    "            print(f\"Successfully processed {len(processed_list)} jobs and saved to 'jobs_processed'.\")\n",
    "\n",
    "    # scrape the job detail page and add must-have skill set into each job\n",
    "    def scrape_must_have_skills(self, limit=0):\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.maximize_window()\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "\n",
    "\n",
    "        jobs = list(self.db.jobs_processed.find().limit(limit))\n",
    "        print(f\"Scraping Must-have skills for {len(jobs)} jobs\")\n",
    "\n",
    "        for job in jobs:\n",
    "            url = job.get(\"jump_url\")\n",
    "            if not url:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(2)  # wait for page to render\n",
    "\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                must_section = soup.select_one('section[branch=\"musts\"]')\n",
    "\n",
    "                skills = []\n",
    "                if must_section:\n",
    "                    skill_tags = must_section.select('span[id^=\"item-tag-\"]')\n",
    "                    for tag in skill_tags:\n",
    "                        skills.append(tag.get_text(strip=True).lower())\n",
    "\n",
    "                # Update MongoDB document\n",
    "                self.db.jobs_processed.update_one(\n",
    "                    {\"_id\": job[\"_id\"]},\n",
    "                    {\"$set\": {\"must_have_skills\": skills}}\n",
    "                )\n",
    "\n",
    "                print(f\"‚úî {job['job_title']} ‚Üí {skills}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error scraping {url} : {e}\")\n",
    "\n",
    "        driver.quit()\n",
    "        print(\"Finished scraping Must-have skills.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beb6c9e56c11a623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialise WebScraping instance\n",
      "DB MODE:  local\n",
      " Connecting to Local MongoDB (mongodb://localhost:27017/)...\n",
      "‚úÖ Successfully connected to database: BD_final\n",
      "Clicked 'See more' (1/50)\n",
      "Finished loading or button not found: Message: element click intercepted: Element <button _ngcontent-serverapp-c585432322=\"\" nfjloadmore=\"\" type=\"button\" class=\"tw-btn tw-btn-primary tw-px-8 tw-block tw-btn-xl\" _nghost-serverapp-c3446581009=\"\">...</button> is not clickable at point (1095, 420). Other element would receive the click: <div class=\"cdk-overlay-connected-position-bounding-box\" dir=\"ltr\" style=\"top: 0px; left: 0px; height: 100%; width: 100%;\">...</div>\n",
      "  (Session info: chrome=143.0.7499.192); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#elementclickinterceptedexception\n",
      "Stacktrace:\n",
      "Symbols not available. Dumping unresolved backtrace:\n",
      "\t0x7ff79ef588d5\n",
      "\t0x7ff79ef58930\n",
      "\t0x7ff79ed3165d\n",
      "\t0x7ff79ed919e8\n",
      "\t0x7ff79ed8f40f\n",
      "\t0x7ff79ed8c417\n",
      "\t0x7ff79ed8b338\n",
      "\t0x7ff79ed7ce7f\n",
      "\t0x7ff79edb1fda\n",
      "\t0x7ff79ed7c746\n",
      "\t0x7ff79eddac97\n",
      "\t0x7ff79ed7ac29\n",
      "\t0x7ff79ed7ba93\n",
      "\t0x7ff79f270620\n",
      "\t0x7ff79f26af60\n",
      "\t0x7ff79f2896c6\n",
      "\t0x7ff79ef75dd4\n",
      "\t0x7ff79ef7ed7c\n",
      "\t0x7ff79ef61ff4\n",
      "\t0x7ff79ef621a5\n",
      "\t0x7ff79ef47ed2\n",
      "\t0x7ff9ddaae8d7\n",
      "\t0x7ff9de7cc53c\n",
      "\n",
      "All pages loaded. Capturing final HTML...\n",
      "Raw HTML saved to MongoDB! Document ID: 6962b5f415ec179e1b779334\n"
     ]
    }
   ],
   "source": [
    "# 1.run the WebScrapper\n",
    "# 2.query and save the raw html data into mongodb\n",
    "scraper = WebScraping(query_term='backend')\n",
    "scraper.scrape_save_raw_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "526ddd2c48217030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 job postings in HTML.\n",
      "Successfully processed 20 jobs and saved to 'jobs_processed'.\n"
     ]
    }
   ],
   "source": [
    "# 3.extract data, and parse the job list html into fields and save into db again\n",
    "scraper.process_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9d64dd20253fa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Must-have skills for 20 jobs\n",
      "‚úî Backend Engineer ‚Üí ['backend', 'elixir']\n",
      "‚úî Middle Backend Developer (Java) ‚Üí ['java', 'microservices', 'docker', 'kubernetes', 'spring', 'boot']\n",
      "‚úî Senior Software Engineer (Backend / Infrastructure) - Java, Python, C++ ‚Üí ['c++', 'ux', 'route', 'degree', 'java', 'communication skills']\n",
      "‚úî Regular/Senior Backend Developer (TypeScript, Node.js) ‚Üí ['node.js', 'typescript', 'aws', 'microservices architecture']\n",
      "‚úî Backend Developer ‚Üí ['php', 'ms sql', 'api', 'rest api', 'git', 'gitlab', 'jira', 'grafana', 'cd']\n",
      "‚úî Senior QA Engineer (Backend / API) ‚Üí ['qa', 'api testing', 'grpc', 'testing', 'azure devops']\n",
      "‚úî Senior/Lead Backend Developer ‚Üí ['java', 'spring', 'spring boot', 'ci/cd', 'apm', 'jmeter']\n",
      "‚úî AWS Backend/Full Stack Developer ‚Üí ['aws', 'aws ec2', 'aws s3', 'cloudwatch', 'javascript']\n",
      "‚úî Senior Backend Engineer (Python) ‚Üí ['python', 'ai']\n",
      "‚úî Senior JAVA Backend Developer ‚Üí ['java', 'eda', 'oidc', 'quarkus', 'cqrs', 'saga', 'ddd']\n",
      "‚úî Senior Software Engineer - Backend (Colleague) ‚Üí ['java', 'spring', 'ci/cd', 'sql', 'design system', 'kubernetes', 'docker', 'azure', 'microservices']\n",
      "‚úî Senior Software Engineer ‚Äì Backend (Product Intelligence) ‚Üí ['java', 'spring', 'ci/cd', 'sql', 'design system', 'kubernetes', 'docker', 'azure', 'microservices']\n",
      "‚úî Backend Software Engineer III ‚Üí ['java']\n",
      "‚úî Backend Software Developer ‚Üí ['java', 'rdbms', 'postgresql', 'mysql', 'aws']\n",
      "‚úî (AI-native) Backend Software Engineer - Product Innovation ‚Üí ['llm', 'python', 'rdbms', 'git', 'ai']\n",
      "‚úî Backend and Fullstack Developers (Python) ‚Üí ['python', 'azure devops', 'data pipelines', 'elasticsearch', 'typescript', 'vue.js', 'mvp']\n",
      "‚úî (AI-native) Senior Backend Software Engineer ‚Üí ['java', 'scala', 'python', 'ai', 'sql', 'rdbms', 'git']\n",
      "‚úî Senior Python/Django Backend Engineer ‚Üí ['python', 'django', 'rest api', 'linux', 'git']\n",
      "‚úî Mid / Senior C# .NET Developer (Backend / E-commerce) ‚Üí ['c#', '.net', '.net core', 'qa', 'ms sql server', 'rabbitmq', 'ddd', 'tdd', 'clean code']\n",
      "‚úî Golang Developer | Backend Engineer ‚Üí ['golang']\n",
      "Finished scraping Must-have skills.\n"
     ]
    }
   ],
   "source": [
    "# 4.More details, Scrape Must-have skills from each job page\n",
    "scraper.scrape_must_have_skills()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd47d064-c58c-4775-90ff-347606e46455",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScrapingJustJoin:\n",
    "    def __init__(self, query_term=\"backend\"):\n",
    "        print(\"Initialise WebScrapingJustJoin instance\")\n",
    "        self.query_term = query_term\n",
    "        self.api_url = (\n",
    "            \"https://api.justjoin.it/v2/user-panel/offers/by-cursor\"\n",
    "            \"?cityRadiusKm=30\"\n",
    "            \"&currency=pln\"\n",
    "            \"&from=0\"\n",
    "            \"&itemsCount=100\"\n",
    "            f\"&keywords[]={query_term.replace(' ', '%20')}\"\n",
    "            \"&orderBy=DESC\"\n",
    "            \"&sortBy=published\"\n",
    "        )\n",
    "        self._init_db()\n",
    "\n",
    "    def _init_db(self):\n",
    "        mode = os.getenv(\"MONGO_MODE\", \"local\")\n",
    "        db_name = os.getenv(\"DB_NAME\", \"BD_final\")\n",
    "\n",
    "        if mode == \"atlas\":\n",
    "            uri = os.getenv(\"ATLAS_MONGO_URI\")\n",
    "        else:\n",
    "            uri = \"mongodb://localhost:27017/\"\n",
    "\n",
    "        self.client = MongoClient(uri)\n",
    "        self.db = self.client[db_name]\n",
    "        print(\"‚úÖ JustJoin connected to DB\")\n",
    "\n",
    "    # ---- salary normalization (matches NoFluff logic) ----\n",
    "    def normalize_salary(self, emp):\n",
    "        if not emp or emp.get(\"from\") is None:\n",
    "            return None, None\n",
    "\n",
    "        from_sal = emp.get(\"from\")\n",
    "        to_sal = emp.get(\"to\")\n",
    "        unit = emp.get(\"unit\")  # hour / day / month\n",
    "\n",
    "        if unit == \"hour\":\n",
    "            return from_sal * 160, to_sal * 160\n",
    "        if unit == \"day\":\n",
    "            return from_sal * 20, to_sal * 20\n",
    "\n",
    "        return from_sal, to_sal  # month\n",
    "\n",
    "    def scrape_and_process(self):\n",
    "        import requests\n",
    "\n",
    "        response = requests.get(self.api_url)\n",
    "        raw = response.json()\n",
    "\n",
    "        processed = []\n",
    "\n",
    "        for job in raw.get(\"data\", []):\n",
    "            emp = job.get(\"employmentTypes\", [{}])[0]\n",
    "            min_sal, max_sal = self.normalize_salary(emp)\n",
    "\n",
    "            processed.append({\n",
    "                \"source\": \"justjoin\",                   \n",
    "                \"job_title\": job.get(\"title\"),\n",
    "                \"company_name\": job.get(\"companyName\"),\n",
    "                \"min_salary\": min_sal,\n",
    "                \"max_salary\": max_sal,\n",
    "                \"location\": job.get(\"city\"),\n",
    "                \"jump_url\": f\"https://justjoin.it/offers/{job.get('slug')}\",\n",
    "                \"must_have_skills\": [\n",
    "                    skill.lower() for skill in job.get(\"requiredSkills\", [])\n",
    "                ],\n",
    "                \"processed_at\": datetime.now(),\n",
    "                \"query_term\": self.query_term\n",
    "            })\n",
    "\n",
    "        self.db.jobs_processed_jj.drop()\n",
    "        self.db.jobs_processed_jj.insert_many(processed)\n",
    "        print(f\"‚úÖ Saved {len(processed)} JustJoin jobs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45595f4c-a5db-4ae5-8fd1-e25fa22a55c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialise WebScrapingJustJoin instance\n",
      "‚úÖ JustJoin connected to DB\n",
      "‚úÖ Saved 100 JustJoin jobs\n"
     ]
    }
   ],
   "source": [
    "# 6Ô∏è‚É£ Scrape JustJoin.it jobs \n",
    "jj_scraper = WebScrapingJustJoin(query_term=\"backend\")\n",
    "jj_scraper.scrape_and_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afdc500-3090-456f-ad4d-b5c6e5e79fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
