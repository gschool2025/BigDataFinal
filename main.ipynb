{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5631c6ab545f4fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T10:50:51.279042Z",
     "start_time": "2026-01-11T10:50:50.858207Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL #1: Setup & Import Dependencies\n",
    "# Description: Install required packages and import libraries\n",
    "# ============================================================\n",
    "\n",
    "# !pip install selenium\n",
    "# !pip install pymongo\n",
    "# !pip install numpy\n",
    "# !pip install BeautifulSoup\n",
    "# !pip install python-dotenv pymongo dnspython\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc5d098dbc78406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T10:50:51.304445Z",
     "start_time": "2026-01-11T10:50:51.287437Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL #2: Define NoFluffJobs Web Scraper Class\n",
    "# Description: Class for scraping job data from NoFluffJobs\n",
    "# Includes methods for HTML scraping, parsing, and MongoDB storage\n",
    "# ============================================================\n",
    "\n",
    "# load environment variable to init db\n",
    "load_dotenv()\n",
    "\n",
    "# HTML webpage scrapping\n",
    "class WebScrapingNoFluff:\n",
    "    def __init__(self, query_term='backend'):\n",
    "        print('Initialise WebScraping instance')\n",
    "        self.query_term = query_term\n",
    "        self.target_url = f\"https://nofluffjobs.com/pl/?lang=en&criteria=jobPosition%3D{self.query_term}\"\n",
    "        self.final_html = ''\n",
    "        # --- init db: local db / Atlas Mongodb ---\n",
    "        self._init_db()\n",
    "\n",
    "\n",
    "    def _init_db(self):\n",
    "        \"\"\"\n",
    "        You don't need .env, cause you will deploy on local db\n",
    "        MONGO_MODE from .env:\n",
    "        \"local\": connect with local mongodb\n",
    "        \"atlas\": connect with remote mongodb\n",
    "        \"\"\"\n",
    "\n",
    "        mode = os.getenv(\"MONGO_MODE\", \"local\")\n",
    "        db_name = os.getenv(\"DB_NAME\", \"BD_final\")\n",
    "\n",
    "        print(\"DB MODE: \", mode)\n",
    "\n",
    "        if mode == \"atlas\":\n",
    "            uri = os.getenv(\"ATLAS_MONGO_URI\")\n",
    "            if not uri:\n",
    "                raise ValueError(\"‚ùå Error: ATLAS_MONGO_URI not found in .env while mode is 'atlas'\")\n",
    "            print(f\"üåê Connecting to Remote MongoDB Atlas...\")\n",
    "        else:\n",
    "            uri = \"mongodb://localhost:27017/\"\n",
    "            print(f\" Connecting to Local MongoDB ({uri})...\")\n",
    "\n",
    "        try:\n",
    "            self.client = MongoClient(uri)\n",
    "            # test connection\n",
    "            self.client.admin.command('ping')\n",
    "            self.db = self.client[db_name]\n",
    "            print(f\"‚úÖ Successfully connected to database: {db_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Database connection failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def scrape_save_raw_to_db(self, clicks=3):\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(self.target_url)\n",
    "        driver.maximize_window()\n",
    "        # Give you 15 seconds to accept/cancel all popup window\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "\n",
    "\n",
    "        # Loop to click \"See more offers\" button 10 times\n",
    "        count = 0\n",
    "        while count < clicks:\n",
    "            try:\n",
    "                # Find the button with the nfjloadmore attribute\n",
    "                load_more_btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[nfjloadmore]\")))\n",
    "\n",
    "                # Scroll to the button position to ensure it is in the viewport\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more_btn)\n",
    "                time.sleep(1.5) # Allow some buffer time for scrolling\n",
    "\n",
    "                # Click to load more\n",
    "                if load_more_btn:\n",
    "                    load_more_btn.click()\n",
    "                count += 1\n",
    "                print(f\"Clicked 'See more' ({count}/{clicks})\")\n",
    "\n",
    "                # Delay for new content to load\n",
    "                time.sleep(2.5)\n",
    "            except Exception as e:\n",
    "                print(f\"Finished loading or button not found: {e}\")\n",
    "                break\n",
    "\n",
    "        # C. Get the complete HTML after 10 clicks and save it\n",
    "        print(\"All pages loaded. Capturing final HTML...\")\n",
    "        self.final_html = driver.page_source\n",
    "        self.save_raw_to_mongodb()\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "    def save_raw_to_mongodb(self):\n",
    "        \"\"\"\n",
    "        Reference your initial logic to store the raw HTML into NoSQL\n",
    "        \"\"\"\n",
    "        # only hold one raw_json html data\n",
    "        self.db.jobs_raw.drop()\n",
    "        website_document = {\n",
    "            'url': self.target_url,\n",
    "            'content': self.final_html,\n",
    "            'date': datetime.now(),\n",
    "            'query_term': self.query_term\n",
    "        }\n",
    "\n",
    "        # Store in the jobs_raw collection\n",
    "        result = self.db.jobs_raw.insert_one(website_document)\n",
    "        print(f\"Raw HTML saved to MongoDB! Document ID: {result.inserted_id}\")\n",
    "\n",
    "    # parse and split salary to min and max two value\n",
    "    def parse_salary(self, salary_str):\n",
    "        \"\"\"\n",
    "        Internal helper method: Parse salary string\n",
    "        Example input: \"10 000 - 15 000 PLN\", \"20 000 PLN\", \"Undisclosed\"\n",
    "        Output: (min_salary, max_salary)\n",
    "        \"\"\"\n",
    "        if not salary_str or \"Undisclosed\" in salary_str or \"Agreement\" in salary_str:\n",
    "            return None, None\n",
    "\n",
    "        # Remove spaces and thousands separators\n",
    "        clean_str = salary_str.replace('\\xa0', '').replace(' ', '')\n",
    "\n",
    "        # Match all digits\n",
    "        numbers = re.findall(r'\\d+', clean_str)\n",
    "\n",
    "        try:\n",
    "            if len(numbers) >= 2:\n",
    "                # Range salary: [10000, 15000]\n",
    "                return float(numbers[0]), float(numbers[1])\n",
    "            elif len(numbers) == 1:\n",
    "                # Fixed salary: [20000]\n",
    "                return float(numbers[0]), float(numbers[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return None, None\n",
    "\n",
    "    # parse job location\n",
    "    def parse_location(self, card):\n",
    "        \"\"\"\n",
    "        Internal helper method: Parse location\n",
    "        \"\"\"\n",
    "        location_tag = card.select_one('span.posting-info__location, nfj-posting-item-city')\n",
    "        if not location_tag:\n",
    "            return \"Unknown\"\n",
    "\n",
    "        loc_text = location_tag.get_text(strip=True)\n",
    "\n",
    "        # Special case handling: Remote work\n",
    "        if \"Remote\" in loc_text or \"Zdalna\" in loc_text:\n",
    "            return \"Remote\"\n",
    "\n",
    "        # Extract city name (some include '+1', filter via split)\n",
    "        city = loc_text.split('+')[0].strip()\n",
    "        return city\n",
    "\n",
    "    # parse job title,company name, Min/Max Salary,Location, Jump URL and save into db\n",
    "    def process_and_save(self):\n",
    "        \"\"\"\n",
    "        Read HTML from jobs_raw and extract fields to store in jobs_processed\n",
    "        extract raw html and parse fields then save into\n",
    "        \"\"\"\n",
    "        # Get the most recently scraped HTML document\n",
    "        raw_data = self.db.jobs_raw.find_one(sort=[(\"date\", -1)])\n",
    "        if not raw_data:\n",
    "            print(\"No raw data found in MongoDB!\")\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(raw_data['content'], 'html.parser')\n",
    "\n",
    "        # Locate all job cards\n",
    "        postings = soup.select('a.posting-list-item')\n",
    "        print(f\"Found {len(postings)} job postings in HTML.\")\n",
    "\n",
    "        processed_list = []\n",
    "        base_domain = \"https://nofluffjobs.com\" # Used to concatenate the full URL\n",
    "\n",
    "        for post in postings:\n",
    "            try:\n",
    "                # --- Field 1: Job Title ---\n",
    "                title_el = post.select_one('h3.posting-title__position, .posting-title__can-hide')\n",
    "                if title_el:\n",
    "                    # Find and remove any potential \"NEW\" tags or other badges\n",
    "                    # NFJ's badge class names usually contain title-badge\n",
    "                    for badge in title_el.select('.title-badge, .title-badge--new'):\n",
    "                        badge.decompose()\n",
    "                    job_title = title_el.get_text(strip=True)\n",
    "                else:\n",
    "                    job_title = \"N/A\"\n",
    "                # print('JOB TITLE:', job_title)\n",
    "\n",
    "                # --- Field 2: Company Name ---\n",
    "                company_el = post.select_one('span.d-block, .company-name')\n",
    "                company_name = company_el.get_text(strip=True) if company_el else \"N/A\"\n",
    "\n",
    "                # --- Fields 3 & 4: Min/Max Salary ---\n",
    "                salary_el = post.select_one('span.text-truncate, nfj-posting-item-salary')\n",
    "                salary_str = salary_el.get_text(strip=True) if salary_el else \"\"\n",
    "                min_sal, max_sal = self.parse_salary(salary_str)\n",
    "\n",
    "                # --- Field 5: Location ---\n",
    "                location = self.parse_location(post)\n",
    "\n",
    "                # --- Field 6: Jump URL ---\n",
    "                # Get relative path from the <a> tag's href attribute and concatenate domain\n",
    "                relative_url = post.get('href')\n",
    "                jump_url = base_domain + relative_url if relative_url else \"N/A\"\n",
    "\n",
    "                # Construct document\n",
    "                job_doc = {\n",
    "                    \"source\": \"nofluffjobs\",\n",
    "                    'job_title': job_title,\n",
    "                    'company_name': company_name,\n",
    "                    'min_salary': min_sal,\n",
    "                    'max_salary': max_sal,\n",
    "                    'location': location,\n",
    "                    'jump_url': jump_url, # Add to the document\n",
    "                    'processed_at': datetime.now(),\n",
    "                    'query_term':raw_data['query_term']\n",
    "                }\n",
    "\n",
    "                processed_list.append(job_doc)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing a single post: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Batch save to the new collection\n",
    "        if processed_list:\n",
    "            self.db.jobs_processed.drop()\n",
    "            self.db.jobs_processed.insert_many(processed_list)\n",
    "            print(f\"Successfully processed {len(processed_list)} jobs and saved to 'jobs_processed'.\")\n",
    "\n",
    "    # scrape the job detail page and add must-have skill set into each job\n",
    "    def scrape_must_have_skills(self, limit=0):\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.maximize_window()\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "\n",
    "\n",
    "        jobs = list(self.db.jobs_processed.find().limit(limit))\n",
    "        print(f\"Scraping Must-have skills for {len(jobs)} jobs\")\n",
    "\n",
    "        for job in jobs:\n",
    "            url = job.get(\"jump_url\")\n",
    "            if not url:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(2)  # wait for page to render\n",
    "\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                must_section = soup.select_one('section[branch=\"musts\"]')\n",
    "\n",
    "                skills = []\n",
    "                if must_section:\n",
    "                    skill_tags = must_section.select('span[id^=\"item-tag-\"]')\n",
    "                    for tag in skill_tags:\n",
    "                        skills.append(tag.get_text(strip=True).lower())\n",
    "\n",
    "                # Update MongoDB document\n",
    "                self.db.jobs_processed.update_one(\n",
    "                    {\"_id\": job[\"_id\"]},\n",
    "                    {\"$set\": {\"must_have_skills\": skills}}\n",
    "                )\n",
    "\n",
    "                print(f\"‚úî {job['job_title']} ‚Üí {skills}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error scraping {url} : {e}\")\n",
    "\n",
    "        driver.quit()\n",
    "        print(\"Finished scraping Must-have skills.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af751c7faeacb7b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T10:51:09.280760Z",
     "start_time": "2026-01-11T10:50:51.313942Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL #3: Execute NoFluffJobs Scraper\n",
    "# Description: Run the web scraper to collect raw HTML data\n",
    "# from NoFluffJobs and save it to MongoDB\n",
    "# ============================================================\n",
    "\n",
    "scraperNoFluff = WebScrapingNoFluff(query_term='backend')\n",
    "scraperNoFluff.scrape_save_raw_to_db()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d1a8c82f514bad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T10:51:09.676123Z",
     "start_time": "2026-01-11T10:51:09.286817Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL #4: Parse and Extract Structured Job Data\n",
    "# Description: Extract job title, company, salary, location\n",
    "# from raw HTML and save to MongoDB jobs_processed collection\n",
    "# ============================================================\n",
    "\n",
    "scraperNoFluff.process_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10801a8fc930e5e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T10:55:25.972455Z",
     "start_time": "2026-01-11T10:51:09.684351Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL #5: Scrape Must-Have Skills\n",
    "# Description: Visit each job detail page and extract\n",
    "# the must-have skills list, adding to MongoDB documents\n",
    "# ============================================================\n",
    "\n",
    "scraperNoFluff.scrape_must_have_skills()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140ca1013a11803f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T10:55:25.999607Z",
     "start_time": "2026-01-11T10:55:25.991839Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL #6: Define JustJoin.it API Scraper Class\n",
    "# Description: Class for scraping job data from JustJoin.it\n",
    "# using their public API (faster than HTML scraping)\n",
    "# ============================================================\n",
    "\n",
    "class WebScrapingJustJoin:\n",
    "    def __init__(self, query_term=\"backend\"):\n",
    "        print(\"Initialise WebScrapingJustJoin instance\")\n",
    "        self.query_term = query_term\n",
    "        self.api_url = (\n",
    "            \"https://api.justjoin.it/v2/user-panel/offers/by-cursor\"\n",
    "            \"?cityRadiusKm=30\"\n",
    "            \"&currency=pln\"\n",
    "            \"&from=0\"\n",
    "            \"&itemsCount=100\"\n",
    "            f\"&keywords[]={query_term.replace(' ', '%20')}\"\n",
    "            \"&orderBy=DESC\"\n",
    "            \"&sortBy=published\"\n",
    "        )\n",
    "        self._init_db()\n",
    "\n",
    "    def _init_db(self):\n",
    "        mode = os.getenv(\"MONGO_MODE\", \"local\")\n",
    "        db_name = os.getenv(\"DB_NAME\", \"BD_final\")\n",
    "\n",
    "        if mode == \"atlas\":\n",
    "            uri = os.getenv(\"ATLAS_MONGO_URI\")\n",
    "        else:\n",
    "            uri = \"mongodb://localhost:27017/\"\n",
    "\n",
    "        self.client = MongoClient(uri)\n",
    "        self.db = self.client[db_name]\n",
    "        print(\"‚úÖ JustJoin connected to DB\")\n",
    "\n",
    "    # ---- salary normalization (matches NoFluff logic) ----\n",
    "    def normalize_salary(self, emp):\n",
    "        if not emp or emp.get(\"from\") is None:\n",
    "            return None, None\n",
    "\n",
    "        from_sal = emp.get(\"from\")\n",
    "        to_sal = emp.get(\"to\")\n",
    "        unit = emp.get(\"unit\")  # hour / day / month\n",
    "\n",
    "        if unit == \"hour\":\n",
    "            return from_sal * 160, to_sal * 160\n",
    "        if unit == \"day\":\n",
    "            return from_sal * 20, to_sal * 20\n",
    "\n",
    "        return from_sal, to_sal  # month\n",
    "\n",
    "    def scrape_and_process(self):\n",
    "        import requests\n",
    "\n",
    "        processed = []\n",
    "        items_per_request = 100   # max jobs per API request\n",
    "        max_items = 300           # total jobs we want\n",
    "\n",
    "        for start in range(0, max_items, items_per_request):\n",
    "            paged_url = (\n",
    "                \"https://api.justjoin.it/v2/user-panel/offers/by-cursor\"\n",
    "                \"?cityRadiusKm=30\"\n",
    "                \"&currency=pln\"\n",
    "                f\"&from={start}\"\n",
    "                f\"&itemsCount={items_per_request}\"\n",
    "                f\"&keywords[]={self.query_term.replace(' ', '%20')}\"\n",
    "                \"&orderBy=DESC\"\n",
    "                \"&sortBy=published\"\n",
    "            )\n",
    "\n",
    "            response = requests.get(paged_url)\n",
    "            raw = response.json()\n",
    "\n",
    "            for job in raw.get(\"data\", []):\n",
    "                emp = job.get(\"employmentTypes\", [{}])[0]\n",
    "                min_sal, max_sal = self.normalize_salary(emp)\n",
    "\n",
    "                processed.append({\n",
    "                    \"source\": \"justjoin\",\n",
    "                    \"job_title\": job.get(\"title\"),\n",
    "                    \"company_name\": job.get(\"companyName\"),\n",
    "                    \"min_salary\": min_sal,\n",
    "                    \"max_salary\": max_sal,\n",
    "                    \"location\": job.get(\"city\"),\n",
    "                    \"jump_url\": f\"https://justjoin.it/offers/{job.get('slug')}\",\n",
    "                    \"must_have_skills\": [\n",
    "                        skill.lower() for skill in job.get(\"requiredSkills\", [])\n",
    "                    ],\n",
    "                    \"processed_at\": datetime.now(),\n",
    "                    \"query_term\": self.query_term\n",
    "\n",
    "                })        \n",
    "        print(f\"‚úÖ Saved {len(processed)} JustJoin jobs\")\n",
    "\n",
    "        self.db.jobs_processed_jj.drop()\n",
    "        self.db.jobs_processed_jj.insert_many(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab423b2a64b3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T10:55:26.820515Z",
     "start_time": "2026-01-11T10:55:26.022217Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL #7: Execute JustJoin.it API Scraper\n",
    "# Description: Scrape jobs from JustJoin.it and save to\n",
    "# MongoDB jobs_processed_jj collection (includes skills)\n",
    "# ============================================================\n",
    "\n",
    "jj_scraper = WebScrapingJustJoin(query_term=\"backend\")\n",
    "jj_scraper.scrape_and_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23187012b6451adf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T10:55:26.828952Z",
     "start_time": "2026-01-11T10:55:26.825409Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL #8: Load Jobs from Both Sources into DataFrame\n",
    "# Description: Retrieves job data from both NoFluffJobs and \n",
    "# JustJoin.it collections, combines them into a pandas DataFrame\n",
    "# with standardized skills formatting\n",
    "# ============================================================\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"BD_final\"]\n",
    "\n",
    "# combine both sources into one dataset\n",
    "jobs_nf = list(db.jobs_processed.find({\"must_have_skills\": {\"$exists\": True, \"$ne\": []}}))\n",
    "jobs_jj = list(db.jobs_processed_jj.find({\"must_have_skills\": {\"$exists\": True, \"$ne\": []}}))\n",
    "\n",
    "jobs = jobs_nf + jobs_jj\n",
    "\n",
    "# Preserve all relevant metadata, including jump_url and query_term\n",
    "df = pd.DataFrame(jobs)[[\n",
    "    \"job_title\",\n",
    "    \"company_name\",\n",
    "    \"must_have_skills\",\n",
    "    \"min_salary\",\n",
    "    \"max_salary\",\n",
    "    \"source\"\n",
    "]]\n",
    "\n",
    "# Drop rows with empty or missing skills\n",
    "df = df[df[\"must_have_skills\"].apply(lambda x: isinstance(x, list) and len(x) > 0)].copy()\n",
    "\n",
    "# Lowercase all skills and join into a single string per job\n",
    "df[\"skills_text\"] = df[\"must_have_skills\"].apply(lambda x: \" \".join([s.lower() for s in x]))\n",
    "\n",
    "print(f\"Loaded {len(df)} jobs with skills\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef671bb-cc12-48b1-b732-9a2ade934d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL #9: Skill Frequency Analysis with MongoDB Aggregation\n",
    "# Description: Performs advanced aggregation pipeline to analyze\n",
    "# skill frequency, salary ranges, and job title distribution\n",
    "# across both job sources\n",
    "# ============================================================\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Connect to MongoDB (adjust the connection string as needed)\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['BD_final']  \n",
    "\n",
    "# Define the pipeline without the unionWith operation\n",
    "pipeline = [\n",
    "    # 1. Remove jobs without skills \n",
    "    {\n",
    "        \"$match\": {\n",
    "            \"must_have_skills\": {\"$exists\": True, \"$ne\": []}\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # 2. Explode skills array ‚Üí one document per skill\n",
    "    {\n",
    "        \"$unwind\": \"$must_have_skills\"\n",
    "    },\n",
    "\n",
    "    # 3. Group by skill\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$must_have_skills\",\n",
    "\n",
    "            # how many job postings mention this skill\n",
    "            \"job_count\": {\"$sum\": 1},\n",
    "\n",
    "            # salary context (Mongo ignores nulls automatically)\n",
    "            \"avg_min_salary\": {\"$avg\": \"$min_salary\"},\n",
    "            \"avg_max_salary\": {\"$avg\": \"$max_salary\"},\n",
    "\n",
    "            # keep example values (useful for inspection)\n",
    "            \"example_titles\": {\"$addToSet\": \"$job_title\"}\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # 4. Add derived fields\n",
    "    {\n",
    "        \"$addFields\": {\n",
    "            \"unique_titles_count\": {\"$size\": \"$example_titles\"}\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run pipeline on each collection separately\n",
    "skill_analysis_1 = list(db.jobs_processed.aggregate(pipeline))\n",
    "skill_analysis_2 = list(db.jobs_processed_jj.aggregate(pipeline))\n",
    "\n",
    "# Combine results properly by merging skills\n",
    "combined_skills = defaultdict(lambda: {\n",
    "    \"job_count\": 0,\n",
    "    \"salary_sum_min\": 0,\n",
    "    \"salary_sum_max\": 0,\n",
    "    \"salary_count\": 0,\n",
    "    \"example_titles\": set()\n",
    "})\n",
    "\n",
    "# Process first collection results\n",
    "for skill in skill_analysis_1:\n",
    "    skill_name = skill[\"_id\"]\n",
    "    combined_skills[skill_name][\"job_count\"] += skill[\"job_count\"]\n",
    "    \n",
    "    # Handle salary data\n",
    "    if \"avg_min_salary\" in skill and skill[\"avg_min_salary\"] is not None:\n",
    "        combined_skills[skill_name][\"salary_sum_min\"] += skill[\"avg_min_salary\"] * skill[\"job_count\"]\n",
    "        combined_skills[skill_name][\"salary_count\"] += skill[\"job_count\"]\n",
    "    \n",
    "    if \"avg_max_salary\" in skill and skill[\"avg_max_salary\"] is not None:\n",
    "        combined_skills[skill_name][\"salary_sum_max\"] += skill[\"avg_max_salary\"] * skill[\"job_count\"]\n",
    "    \n",
    "    # Add example titles\n",
    "    combined_skills[skill_name][\"example_titles\"].update(skill[\"example_titles\"])\n",
    "\n",
    "# Process second collection results\n",
    "for skill in skill_analysis_2:\n",
    "    skill_name = skill[\"_id\"]\n",
    "    combined_skills[skill_name][\"job_count\"] += skill[\"job_count\"]\n",
    "    \n",
    "    # Handle salary data\n",
    "    if \"avg_min_salary\" in skill and skill[\"avg_min_salary\"] is not None:\n",
    "        combined_skills[skill_name][\"salary_sum_min\"] += skill[\"avg_min_salary\"] * skill[\"job_count\"]\n",
    "        combined_skills[skill_name][\"salary_count\"] += skill[\"job_count\"]\n",
    "    \n",
    "    if \"avg_max_salary\" in skill and skill[\"avg_max_salary\"] is not None:\n",
    "        combined_skills[skill_name][\"salary_sum_max\"] += skill[\"avg_max_salary\"] * skill[\"job_count\"]\n",
    "    \n",
    "    # Add example titles\n",
    "    combined_skills[skill_name][\"example_titles\"].update(skill[\"example_titles\"])\n",
    "\n",
    "# Convert to final format\n",
    "skill_analysis = []\n",
    "for skill_name, data in combined_skills.items():\n",
    "    skill_doc = {\n",
    "        \"_id\": skill_name,\n",
    "        \"job_count\": data[\"job_count\"],\n",
    "        \"example_titles\": list(data[\"example_titles\"]),\n",
    "        \"unique_titles_count\": len(data[\"example_titles\"])\n",
    "    }\n",
    "    \n",
    "    # Calculate average salaries if data exists\n",
    "    if data[\"salary_count\"] > 0:\n",
    "        skill_doc[\"avg_min_salary\"] = data[\"salary_sum_min\"] / data[\"salary_count\"]\n",
    "        skill_doc[\"avg_max_salary\"] = data[\"salary_sum_max\"] / data[\"salary_count\"]\n",
    "    \n",
    "    skill_analysis.append(skill_doc)\n",
    "\n",
    "# Sort by job count (most in-demand skills first)\n",
    "skill_analysis.sort(key=lambda x: x[\"job_count\"], reverse=True)\n",
    "\n",
    "# Quick inspection\n",
    "for doc in skill_analysis[:10]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd75320-2652-4df9-a785-b56141888569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL #10: TF-IDF Vectorization of Skills\n",
    "# Description: Transforms job skills into a numerical matrix using\n",
    "# TF-IDF vectorization for subsequent analysis and modeling\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    min_df=0.015,   # ignore very rare skills\n",
    "    max_df=0.99\n",
    ")\n",
    "\n",
    "X_skills = vectorizer.fit_transform(df[\"skills_text\"])\n",
    "print(\"Skill matrix shape:\", X_skills.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0407f133-e37f-406d-83a4-3d0cc8abf6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL #11: Determine Optimal Number of Clusters\n",
    "# Description: Uses elbow method and silhouette scores to identify\n",
    "# the optimal number of skill clusters for backend job market\n",
    "# ============================================================\n",
    "\n",
    "# Estimate reasonable number of clusters for backend job skills \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# Count unique skill combinations\n",
    "df[\"skills_tuple\"] = df[\"must_have_skills\"].apply(lambda x: tuple(sorted(x)))\n",
    "skill_combos = df.groupby(\"skills_tuple\").size().sort_values(ascending=False)\n",
    "\n",
    "print(f\"Total jobs: {len(df)}\")\n",
    "print(f\"Total unique skill combinations: {len(skill_combos)}\")\n",
    "print(\"Top 10 most common skill combinations:\")\n",
    "print(skill_combos.head(10))\n",
    "\n",
    "# Heuristic for number of clusters\n",
    "heuristic_k = int(np.sqrt(len(df)/2))\n",
    "print(f\"\\nHeuristic suggestion for max number of clusters: {heuristic_k}\")\n",
    "\n",
    "# Silhouette scores for k=2 to 9\n",
    "print(\"\\nSilhouette scores for different k:\")\n",
    "for k in range(2, 20):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)\n",
    "    labels = kmeans.fit_predict(X_skills)\n",
    "    score = silhouette_score(X_skills, labels)\n",
    "    print(f\"k={k}, silhouette score={score:.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Use this information to pick an interpretable k\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e33b3b-c9df-453f-87b5-2acacff63ccb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL #12: Determine Optimal Number of Clusters\n",
    "# Description: Uses elbow method and silhouette scores to identify\n",
    "# the optimal number of skill clusters for backend job market\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Standardize X_skills (optional, can use as-is since TF-IDF is normalized)\n",
    "X_dense = X_skills.toarray()\n",
    "\n",
    "K_range = range(2, 20)\n",
    "distortions = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)\n",
    "    labels = kmeans.fit_predict(X_dense)\n",
    "    \n",
    "    # distortion: mean distance to closest centroid\n",
    "    dist = np.mean(np.min(cdist(X_dense, kmeans.cluster_centers_, 'euclidean'), axis=1))\n",
    "    distortions.append(dist)\n",
    "    \n",
    "    sil = silhouette_score(X_dense, labels)\n",
    "    silhouette_scores.append(sil)\n",
    "    \n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(K_range, distortions, 'bx-', linewidth=2)\n",
    "plt.xlabel(\"Number of clusters k\")\n",
    "plt.ylabel(\"Distortion\")\n",
    "plt.title(\"Elbow Method (Distortion)\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(K_range, silhouette_scores, 'ro-', linewidth=2)\n",
    "plt.xlabel(\"Number of clusters k\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Score per k\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173cb2c1-0e3a-46c6-bc2f-c989a798378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL #13: Apply K-Means Clustering and Analyze Skill Groups\n",
    "# Description: Implements final clustering with optimal k=12,\n",
    "# identifies key skills in each cluster, and provides examples\n",
    "# of representative job titles for each skill group\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Use the optimal k=12 based on silhouette analysis\n",
    "k_optimal = 12\n",
    "\n",
    "# Apply KMeans with the optimal k\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = kmeans.fit_predict(X_dense)\n",
    "\n",
    "# Get cluster sizes\n",
    "cluster_sizes = df[\"cluster\"].value_counts().sort_index()\n",
    "print(f\"Jobs per cluster:\")\n",
    "for i in range(k_optimal):\n",
    "    print(f\"Cluster {i}: {cluster_sizes.get(i, 0)} jobs\")\n",
    "\n",
    "# Extract feature names from the vectorizer\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display top skills for each cluster\n",
    "print(\"\\nTop skills per cluster:\")\n",
    "for cluster_id in range(k_optimal):\n",
    "    center = kmeans.cluster_centers_[cluster_id]\n",
    "    top_idx = np.argsort(center)[-10:][::-1]\n",
    "    top_skills = [terms[i] for i in top_idx]\n",
    "    \n",
    "    # Get the weights for the top skills\n",
    "    top_weights = center[top_idx]\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id} ({cluster_sizes.get(cluster_id, 0)} jobs):\")\n",
    "    # Print skills with their weights\n",
    "    for skill, weight in zip(top_skills, top_weights):\n",
    "        print(f\"  - {skill}: {weight:.3f}\")\n",
    "    \n",
    "    # Show example job titles from this cluster\n",
    "    if cluster_id in cluster_sizes.index:\n",
    "        examples = df[df[\"cluster\"] == cluster_id][\"job_title\"].sample(min(3, cluster_sizes[cluster_id])).tolist()\n",
    "        print(f\"  Example jobs: {examples}\")\n",
    "\n",
    "print(\"\\n‚úÖ Skill clusters identified with their most representative technologies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55969485-5aab-4e32-a88a-ced628934210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL #14: Salary Analysis by Cluster\n",
    "# Description: Analyzes salary distributions across skill clusters\n",
    "# to identify high-value skill combinations in the job market\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Filter to jobs with salary data\n",
    "salary_df = df[df[\"min_salary\"].notna() & df[\"max_salary\"].notna()].copy()\n",
    "\n",
    "# Calculate average salary for each job\n",
    "salary_df[\"avg_salary\"] = (salary_df[\"min_salary\"] + salary_df[\"max_salary\"]) / 2\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR  # Standard outlier threshold\n",
    "    \n",
    "    return df[df[column] <= upper_bound]\n",
    "\n",
    "# Apply outlier removal\n",
    "salary_df_filtered = remove_outliers(salary_df, \"avg_salary\")\n",
    "print(f\"Removed {len(salary_df) - len(salary_df_filtered)} outliers from {len(salary_df)} jobs with salary data\")\n",
    "\n",
    "# 1. Summary statistics by cluster using filtered data\n",
    "cluster_salary_stats = salary_df_filtered.groupby(\"cluster\")[\"avg_salary\"].agg(\n",
    "    [\"count\", \"mean\", \"median\", \"std\", \"min\", \"max\"]\n",
    ").sort_values(by=\"median\", ascending=False)\n",
    "\n",
    "print(\"Salary Statistics by Cluster (sorted by median salary):\")\n",
    "print(cluster_salary_stats)\n",
    "\n",
    "# 2. Visualize salary distributions with filtered data\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x=\"cluster\", y=\"avg_salary\", data=salary_df_filtered, order=cluster_salary_stats.index)\n",
    "plt.title(\"Salary Distribution by Skill Cluster (Outliers Removed)\", fontsize=16)\n",
    "plt.xlabel(\"Cluster\", fontsize=14)\n",
    "plt.ylabel(\"Average Salary (PLN)\", fontsize=14)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add cluster labels below the x-axis\n",
    "cluster_labels = []\n",
    "for cluster_id in cluster_salary_stats.index:\n",
    "    # Get top 3 skills for this cluster\n",
    "    center = kmeans.cluster_centers_[cluster_id]\n",
    "    top_idx = np.argsort(center)[-3:][::-1]\n",
    "    top_skills = [terms[i] for i in top_idx]\n",
    "    cluster_labels.append(f\"{cluster_id}: {', '.join(top_skills)}\")\n",
    "\n",
    "plt.xticks(range(len(cluster_labels)), cluster_labels, rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Identify highest and lowest paying clusters\n",
    "highest_paying = cluster_salary_stats.index[0]\n",
    "lowest_paying = cluster_salary_stats.index[-1]\n",
    "\n",
    "print(f\"\\nHighest paying cluster ({highest_paying}):\")\n",
    "top_skills_highest = [terms[i] for i in np.argsort(kmeans.cluster_centers_[highest_paying])[-10:][::-1]]\n",
    "print(f\"Top skills: {', '.join(top_skills_highest)}\")\n",
    "print(f\"Median salary: {cluster_salary_stats.loc[highest_paying, 'median']:.2f} PLN\")\n",
    "\n",
    "print(f\"\\nLowest paying cluster ({lowest_paying}):\")\n",
    "top_skills_lowest = [terms[i] for i in np.argsort(kmeans.cluster_centers_[lowest_paying])[-10:][::-1]]\n",
    "print(f\"Top skills: {', '.join(top_skills_lowest)}\")\n",
    "print(f\"Median salary: {cluster_salary_stats.loc[lowest_paying, 'median']:.2f} PLN\")\n",
    "\n",
    "print(\"\\n‚úÖ Salary analysis complete - identified high-value skill combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ae6ab3-78aa-41b5-92c1-a2a2431e4d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL #15: Skill Gap and Demand Analysis\n",
    "# Description: Identifies valuable skills that appear predominantly\n",
    "# in higher-paying clusters and analyzes demand vs. compensation\n",
    "# ============================================================\n",
    "\n",
    "# Calculate skill importance across clusters\n",
    "skill_importance = {}\n",
    "\n",
    "# Extract feature names from the vectorizer\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# For each cluster, calculate importance of each skill\n",
    "for cluster_id in range(k_optimal):\n",
    "    # Get the cluster center\n",
    "    center = kmeans.cluster_centers_[cluster_id]\n",
    "    \n",
    "    # If this is the first cluster, initialize the dictionary with all skills\n",
    "    if cluster_id == 0:\n",
    "        for i, skill in enumerate(terms):\n",
    "            skill_importance[skill] = [0] * k_optimal\n",
    "    \n",
    "    # Store the importance (weight) of each skill in this cluster\n",
    "    for i, skill in enumerate(terms):\n",
    "        skill_importance[skill][cluster_id] = center[i]\n",
    "\n",
    "# Now skill_importance is a dictionary where:\n",
    "# - Keys are skill names\n",
    "# - Values are lists of importance scores for each cluster\n",
    "\n",
    "print(f\"Skill importance calculated for {len(skill_importance)} skills across {k_optimal} clusters\")\n",
    "\n",
    "# Optional: You can also calculate overall importance by summing or averaging across clusters\n",
    "overall_importance = {skill: sum(scores) for skill, scores in skill_importance.items()}\n",
    "\n",
    "# Example: Show top 10 skills by overall importance\n",
    "top_skills = sorted(overall_importance.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"\\nTop 10 skills by overall importance:\")\n",
    "for skill, score in top_skills:\n",
    "    print(f\"  - {skill}: {score:.3f}\")\n",
    "    \n",
    "# Use the actual number of clusters from your data\n",
    "k_optimal = len(skill_importance[list(skill_importance.keys())[0]])\n",
    "print(f\"Using k_optimal = {k_optimal}\")\n",
    "\n",
    "# 1. Calculate skill importance across all clusters\n",
    "for term_idx, term in enumerate(terms):\n",
    "    # Get the weight of this skill in each cluster\n",
    "    weights = [center[term_idx] for center in kmeans.cluster_centers_]\n",
    "    skill_importance[term] = weights\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "skill_df = pd.DataFrame(skill_importance).T\n",
    "skill_df.columns = [f\"cluster_{i}\" for i in range(k_optimal)]\n",
    "\n",
    "# 2. Calculate correlation between skill presence and cluster salary\n",
    "cluster_median_salaries = cluster_salary_stats[\"median\"].to_dict()\n",
    "salary_correlation = {}\n",
    "\n",
    "for skill in skill_df.index:\n",
    "    # Weight each cluster's importance by its median salary\n",
    "    weighted_importance = sum(\n",
    "        skill_df.loc[skill, f\"cluster_{c}\"] * cluster_median_salaries.get(c, 0)\n",
    "        for c in range(k_optimal)\n",
    "    )\n",
    "    salary_correlation[skill] = weighted_importance\n",
    "\n",
    "# Sort skills by salary correlation\n",
    "salary_corr_df = pd.DataFrame({\n",
    "    \"skill\": list(salary_correlation.keys()),\n",
    "    \"salary_correlation\": list(salary_correlation.values())\n",
    "}).sort_values(\"salary_correlation\", ascending=False)\n",
    "\n",
    "# 3. Calculate skill demand (frequency across all jobs)\n",
    "skill_demand = {}\n",
    "# Use the same set of skills as in salary_correlation to ensure consistency\n",
    "for skill in salary_correlation.keys():\n",
    "    # Count jobs that require this skill\n",
    "    demand = df[\"skills_text\"].str.contains(r'\\b' + skill + r'\\b').sum()\n",
    "    skill_demand[skill] = demand / len(df)  # Normalize by total jobs\n",
    "\n",
    "# 4. Combine salary correlation and demand\n",
    "skill_analysis = salary_corr_df.copy()\n",
    "skill_analysis[\"demand\"] = skill_analysis[\"skill\"].map(skill_demand)\n",
    "\n",
    "# Filter to skills with meaningful presence (at least 5% of jobs)\n",
    "skill_analysis = skill_analysis[skill_analysis[\"demand\"] >= 0.05].reset_index(drop=True)\n",
    "\n",
    "# Define the t_values function to sort the skill_analysis DataFrame\n",
    "def t_values(column_name, ascending=True):\n",
    "    return skill_analysis.sort_values(by=column_name, ascending=ascending)\n",
    "\n",
    "# 5. Identify high-value skills (high salary correlation, lower demand)\n",
    "print(\"Top 20 High-Value Skills (High Salary Correlation):\")\n",
    "print(skill_analysis.head(20))\n",
    "\n",
    "# 6. Visualize skill value vs. demand with smart label placement for top 20 high-value skills\n",
    "import matplotlib.patheffects as PathEffects\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Create a scatter plot with color gradient based on salary correlation for all points\n",
    "scatter = plt.scatter(\n",
    "    skill_analysis[\"demand\"], \n",
    "    skill_analysis[\"salary_correlation\"],\n",
    "    alpha=0.8,\n",
    "    c=skill_analysis[\"salary_correlation\"],  # Color by salary correlation\n",
    "    cmap='viridis',                          # Use a color gradient\n",
    "    s=80                                     # Slightly larger points\n",
    ")\n",
    "\n",
    "# Add a color bar to show the salary correlation scale\n",
    "plt.colorbar(scatter, label=\"Salary Correlation\")\n",
    "\n",
    "# Get the top 20 skills by salary correlation\n",
    "top20_skills = t_values(\"salary_correlation\", ascending=False).head(20)\n",
    "\n",
    "# Highlight the top 20 skills with a distinct border\n",
    "for i, row in top20_skills.iterrows():\n",
    "    plt.scatter(\n",
    "        row[\"demand\"],\n",
    "        row[\"salary_correlation\"],\n",
    "        s=150,              # Larger size\n",
    "        facecolors='none',  # Transparent fill\n",
    "        edgecolors='white', # White border\n",
    "        linewidths=2,       # Thicker border\n",
    "        zorder=10           # Ensure it's on top\n",
    "    )\n",
    "\n",
    "# Add labels with varying offsets to reduce overlap\n",
    "offsets = [(7, 0), (7, 10), (7, -10), (-40, 0), (-40, 10), (-40, -10), \n",
    "           (7, 20), (7, -20), (-40, 20), (-40, -20)]\n",
    "\n",
    "for i, row in top20_skills.iterrows():\n",
    "    # Use different offsets for different points\n",
    "    offset = offsets[i % len(offsets)]\n",
    "    \n",
    "    # Draw a line from the data point to the label if offset is significant\n",
    "    if abs(offset[0]) > 10 or abs(offset[1]) > 5:\n",
    "        plt.annotate(\n",
    "            '',\n",
    "            xy=(row[\"demand\"], row[\"salary_correlation\"]),\n",
    "            xytext=(row[\"demand\"] + offset[0]/500, row[\"salary_correlation\"] + offset[1]/500),\n",
    "            arrowprops=dict(arrowstyle='-', color='white', lw=1, alpha=0.7),\n",
    "            zorder=9\n",
    "        )\n",
    "    \n",
    "    # Add the label\n",
    "    plt.annotate(\n",
    "        row[\"skill\"],\n",
    "        xy=(row[\"demand\"], row[\"salary_correlation\"]),\n",
    "        xytext=offset,\n",
    "        textcoords=\"offset points\",\n",
    "        fontsize=11,\n",
    "        fontweight='bold',\n",
    "        color='white',\n",
    "        path_effects=[\n",
    "            PathEffects.withStroke(linewidth=3, foreground='black')\n",
    "        ],\n",
    "        va='center',\n",
    "        zorder=11\n",
    "    )\n",
    "\n",
    "plt.title(\"Top 20 High-Value Skills by Salary Correlation\", fontsize=16)\n",
    "plt.xlabel(\"Demand (% of Jobs Requiring Skill)\", fontsize=14)\n",
    "plt.ylabel(\"Salary Correlation\", fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Just print the completion message without the redundant list\n",
    "print(\"\\n‚úÖ Skill gap analysis complete - identified high-value skills with varying demand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4835d9-53a8-472e-a2cb-36820515bdab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
