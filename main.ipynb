{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T15:38:31.294055Z",
     "start_time": "2026-01-05T15:38:31.289169Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install selenium\n",
    "# !pip install pymongo\n",
    "# !pip install numpy\n",
    "# !pip install BeautifulSoup\n",
    "# !pip install python-dotenv pymongo dnspython\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb5150b233b2a92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T15:39:26.491779Z",
     "start_time": "2026-01-05T15:38:31.314070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialise WebScraping instance\n",
      "DB MODE:  local\n",
      " Connecting to Local MongoDB (mongodb://localhost:27017/)...\n",
      "‚úÖ Successfully connected to database: BD_final\n",
      "Finished loading or button not found: Message: element click intercepted: Element <button _ngcontent-serverapp-c467516314=\"\" nfjloadmore=\"\" type=\"button\" class=\"tw-btn tw-btn-primary tw-px-8 tw-block tw-btn-xl\" _nghost-serverapp-c3446581009=\"\" ngh=\"44\">...</button> is not clickable at point (1095, 420). Other element would receive the click: <html lang=\"en\">...</html>\n",
      "  (Session info: chrome=143.0.7499.169); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#elementclickinterceptedexception\n",
      "Stacktrace:\n",
      "Symbols not available. Dumping unresolved backtrace:\n",
      "\t0x7ff7995688e5\n",
      "\t0x7ff799568940\n",
      "\t0x7ff79934165d\n",
      "\t0x7ff7993a19e8\n",
      "\t0x7ff79939f40f\n",
      "\t0x7ff79939c417\n",
      "\t0x7ff79939b338\n",
      "\t0x7ff79938ce7f\n",
      "\t0x7ff7993c1fda\n",
      "\t0x7ff79938c746\n",
      "\t0x7ff7993eac97\n",
      "\t0x7ff79938ac29\n",
      "\t0x7ff79938ba93\n",
      "\t0x7ff799880640\n",
      "\t0x7ff79987af80\n",
      "\t0x7ff7998996e6\n",
      "\t0x7ff799585de4\n",
      "\t0x7ff79958ed8c\n",
      "\t0x7ff799572004\n",
      "\t0x7ff7995721b5\n",
      "\t0x7ff799557ee2\n",
      "\t0x7ffcb5d8e8d7\n",
      "\t0x7ffcb686c53c\n",
      "\n",
      "All pages loaded. Capturing final HTML...\n",
      "Raw HTML saved to MongoDB! Document ID: 695c0f8795eb8ef5034df8c6\n",
      "Found 20 job postings in HTML.\n",
      "Successfully processed 20 jobs and saved to 'jobs_processed'.\n"
     ]
    }
   ],
   "source": [
    "# Please check README.md first\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class WebScraping:\n",
    "    def __init__(self, query_term='business-analyst'):\n",
    "        print('Initialise WebScraping instance')\n",
    "        self.query_term = query_term\n",
    "        self.target_url = f\"https://nofluffjobs.com/pl/?lang=en&criteria=jobPosition%3D{self.query_term}\"\n",
    "        self.final_html = ''\n",
    "        # --- init db: local db / Atlas Mongodb ---\n",
    "        self._init_db()\n",
    "\n",
    "\n",
    "    def _init_db(self):\n",
    "        \"\"\"\n",
    "        You don't need .env, cause you will deploy on local db\n",
    "        MONGO_MODE from .env:\n",
    "        \"local\": connect with local mongodb\n",
    "        \"atlas\": connect with remote mongodb\n",
    "        \"\"\"\n",
    "\n",
    "        mode = os.getenv(\"MONGO_MODE\", \"local\")\n",
    "        db_name = os.getenv(\"DB_NAME\", \"BD_final\")\n",
    "\n",
    "        print(\"DB MODE: \", mode)\n",
    "\n",
    "        if mode == \"atlas\":\n",
    "            uri = os.getenv(\"ATLAS_MONGO_URI\")\n",
    "            if not uri:\n",
    "                raise ValueError(\"‚ùå Error: ATLAS_MONGO_URI not found in .env while mode is 'atlas'\")\n",
    "            print(f\"üåê Connecting to Remote MongoDB Atlas...\")\n",
    "        else:\n",
    "            uri = \"mongodb://localhost:27017/\"\n",
    "            print(f\" Connecting to Local MongoDB ({uri})...\")\n",
    "\n",
    "        try:\n",
    "            self.client = MongoClient(uri)\n",
    "            # ÊµãËØïËøûÊé•\n",
    "            self.client.admin.command('ping')\n",
    "            self.db = self.client[db_name]\n",
    "            print(f\"‚úÖ Successfully connected to database: {db_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Database connection failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def scrape_save_raw_to_db(self, clicks=10):\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.maximize_window()\n",
    "        driver.get(self.target_url)\n",
    "        # Give you 15 seconds to accept/cancel all popup window\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "\n",
    "        # # A. Handle Cookie pop-ups\n",
    "        # try:\n",
    "        #     cookie_btn = wait.until(EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\")))\n",
    "        #     cookie_btn.click()\n",
    "        #     print(\"Cookies accepted.\")\n",
    "        # except:\n",
    "        #     print(\"No cookie banner found or already accepted.\")\n",
    "\n",
    "        # B. Loop to click \"See more offers\" button 10 times\n",
    "        count = 0\n",
    "        while count < clicks:\n",
    "            try:\n",
    "                # Find the button with the nfjloadmore attribute\n",
    "                load_more_btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[nfjloadmore]\")))\n",
    "\n",
    "                # Scroll to the button position to ensure it is in the viewport\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more_btn)\n",
    "                time.sleep(1.5) # Allow some buffer time for scrolling\n",
    "\n",
    "                # Click to load more\n",
    "                if load_more_btn:\n",
    "                    load_more_btn.click()\n",
    "                count += 1\n",
    "                print(f\"Clicked 'See more' ({count}/{clicks})\")\n",
    "\n",
    "                # Delay for new content to load\n",
    "                time.sleep(2.5)\n",
    "            except Exception as e:\n",
    "                print(f\"Finished loading or button not found: {e}\")\n",
    "                break\n",
    "\n",
    "        # C. Get the complete HTML after 10 clicks and save it\n",
    "        print(\"All pages loaded. Capturing final HTML...\")\n",
    "        self.final_html = driver.page_source\n",
    "        self.save_raw_to_mongodb()\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "    def save_raw_to_mongodb(self):\n",
    "        \"\"\"\n",
    "        Reference your initial logic to store the raw HTML into NoSQL\n",
    "        \"\"\"\n",
    "        # only hold one raw_json html data\n",
    "        self.db.jobs_raw.drop()\n",
    "        website_document = {\n",
    "            'url': self.target_url,\n",
    "            'content': self.final_html,\n",
    "            'date': datetime.now(),\n",
    "            'query_term': self.query_term\n",
    "        }\n",
    "\n",
    "        # Store in the jobs_raw collection\n",
    "        result = self.db.jobs_raw.insert_one(website_document)\n",
    "        print(f\"Raw HTML saved to MongoDB! Document ID: {result.inserted_id}\")\n",
    "\n",
    "    def _parse_salary(self, salary_str):\n",
    "        \"\"\"\n",
    "        Internal helper method: Parse salary string\n",
    "        Example input: \"10 000 - 15 000 PLN\", \"20 000 PLN\", \"Undisclosed\"\n",
    "        Output: (min_salary, max_salary)\n",
    "        \"\"\"\n",
    "        if not salary_str or \"Undisclosed\" in salary_str or \"Agreement\" in salary_str:\n",
    "            return None, None\n",
    "\n",
    "        # Remove spaces and thousands separators\n",
    "        clean_str = salary_str.replace('\\xa0', '').replace(' ', '')\n",
    "\n",
    "        # Match all digits\n",
    "        numbers = re.findall(r'\\d+', clean_str)\n",
    "\n",
    "        try:\n",
    "            if len(numbers) >= 2:\n",
    "                # Range salary: [10000, 15000]\n",
    "                return float(numbers[0]), float(numbers[1])\n",
    "            elif len(numbers) == 1:\n",
    "                # Fixed salary: [20000]\n",
    "                return float(numbers[0]), float(numbers[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return None, None\n",
    "\n",
    "    def _parse_location(self, card):\n",
    "        \"\"\"\n",
    "        Internal helper method: Parse location\n",
    "        \"\"\"\n",
    "        location_tag = card.select_one('span.posting-info__location, nfj-posting-item-city')\n",
    "        if not location_tag:\n",
    "            return \"Unknown\"\n",
    "\n",
    "        loc_text = location_tag.get_text(strip=True)\n",
    "\n",
    "        # Special case handling: Remote work\n",
    "        if \"Remote\" in loc_text or \"Zdalna\" in loc_text:\n",
    "            return \"Remote\"\n",
    "\n",
    "        # Extract city name (some include '+1', filter via split)\n",
    "        city = loc_text.split('+')[0].strip()\n",
    "        return city\n",
    "\n",
    "    def process_and_save(self):\n",
    "        \"\"\"\n",
    "        Read HTML from jobs_raw and extract fields to store in jobs_processed\n",
    "        extract raw html and parse fields then save into\n",
    "        \"\"\"\n",
    "        # 1. Get the most recently scraped HTML document\n",
    "        raw_data = self.db.jobs_raw.find_one(sort=[(\"date\", -1)])\n",
    "        if not raw_data:\n",
    "            print(\"No raw data found in MongoDB!\")\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(raw_data['content'], 'html.parser')\n",
    "\n",
    "        # 2. Locate all job cards\n",
    "        postings = soup.select('a.posting-list-item')\n",
    "        print(f\"Found {len(postings)} job postings in HTML.\")\n",
    "\n",
    "        processed_list = []\n",
    "        base_domain = \"https://nofluffjobs.com\" # Used to concatenate the full URL\n",
    "\n",
    "        for post in postings:\n",
    "            try:\n",
    "                # --- Field 1: Job Title ---\n",
    "                title_el = post.select_one('h3.posting-title__position, .posting-title__can-hide')\n",
    "                if title_el:\n",
    "                    # Find and remove any potential \"NEW\" tags or other badges\n",
    "                    # NFJ's badge class names usually contain title-badge\n",
    "                    for badge in title_el.select('.title-badge, .title-badge--new'):\n",
    "                        badge.decompose()\n",
    "                    job_title = title_el.get_text(strip=True)\n",
    "                else:\n",
    "                    job_title = \"N/A\"\n",
    "                # print('JOB TITLE:', job_title)\n",
    "\n",
    "                # --- Field 2: Company Name ---\n",
    "                company_el = post.select_one('span.d-block, .company-name')\n",
    "                company_name = company_el.get_text(strip=True) if company_el else \"N/A\"\n",
    "\n",
    "                # --- Fields 3 & 4: Min/Max Salary ---\n",
    "                salary_el = post.select_one('span.text-truncate, nfj-posting-item-salary')\n",
    "                salary_str = salary_el.get_text(strip=True) if salary_el else \"\"\n",
    "                min_sal, max_sal = self._parse_salary(salary_str)\n",
    "\n",
    "                # --- Field 5: Location ---\n",
    "                location = self._parse_location(post)\n",
    "\n",
    "                # --- Field 6: Jump URL (New) ---\n",
    "                # Get relative path from the <a> tag's href attribute and concatenate domain\n",
    "                relative_url = post.get('href')\n",
    "                jump_url = base_domain + relative_url if relative_url else \"N/A\"\n",
    "\n",
    "                # Construct document\n",
    "                job_doc = {\n",
    "                    'job_title': job_title,\n",
    "                    'company_name': company_name,\n",
    "                    'min_salary': min_sal,\n",
    "                    'max_salary': max_sal,\n",
    "                    'location': location,\n",
    "                    'jump_url': jump_url, # Add to the document\n",
    "                    'processed_at': datetime.now(),\n",
    "                    'query_term':raw_data['query_term']\n",
    "                }\n",
    "\n",
    "                processed_list.append(job_doc)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing a single post: {e}\")\n",
    "                continue\n",
    "\n",
    "        # 3. Batch save to the new collection\n",
    "        if processed_list:\n",
    "            self.db.jobs_processed.drop()\n",
    "            self.db.jobs_processed.insert_many(processed_list)\n",
    "            print(f\"Successfully processed {len(processed_list)} jobs and saved to 'jobs_processed'.\")\n",
    "\n",
    "\n",
    "# run the WebScrapper\n",
    "# query and save the raw html data into mongodb\n",
    "scraper = WebScraping(query_term='business-analyst')\n",
    "scraper.scrape_save_raw_to_db()\n",
    "# extract data, and parse to fields and save into db again\n",
    "scraper.process_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0eb01e-f0d0-40f4-9a2e-fd3578b87687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicked 'See more' (1/5)\n",
      "Finished loading or button not found: Message: element click intercepted: Element <button _ngcontent-serverapp-c467516314=\"\" nfjloadmore=\"\" type=\"button\" class=\"tw-btn tw-btn-primary tw-px-8 tw-block tw-btn-xl\" _nghost-serverapp-c3446581009=\"\">...</button> is not clickable at point (1095, 420). Other element would receive the click: <div class=\"cdk-overlay-connected-position-bounding-box\" dir=\"ltr\" style=\"top: 0px; left: 0px; height: 100%; width: 100%;\">...</div>\n",
      "  (Session info: chrome=143.0.7499.169); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#elementclickinterceptedexception\n",
      "Stacktrace:\n",
      "Symbols not available. Dumping unresolved backtrace:\n",
      "\t0x7ff7995688e5\n",
      "\t0x7ff799568940\n",
      "\t0x7ff79934165d\n",
      "\t0x7ff7993a19e8\n",
      "\t0x7ff79939f40f\n",
      "\t0x7ff79939c417\n",
      "\t0x7ff79939b338\n",
      "\t0x7ff79938ce7f\n",
      "\t0x7ff7993c1fda\n",
      "\t0x7ff79938c746\n",
      "\t0x7ff7993eac97\n",
      "\t0x7ff79938ac29\n",
      "\t0x7ff79938ba93\n",
      "\t0x7ff799880640\n",
      "\t0x7ff79987af80\n",
      "\t0x7ff7998996e6\n",
      "\t0x7ff799585de4\n",
      "\t0x7ff79958ed8c\n",
      "\t0x7ff799572004\n",
      "\t0x7ff7995721b5\n",
      "\t0x7ff799557ee2\n",
      "\t0x7ffcb5d8e8d7\n",
      "\t0x7ffcb686c53c\n",
      "\n",
      "All pages loaded. Capturing final HTML...\n",
      "Raw HTML saved to MongoDB! Document ID: 695c107195eb8ef5034df8db\n"
     ]
    }
   ],
   "source": [
    "# Scrape raw HTML for job listings\n",
    "scraper.scrape_save_raw_to_db(clicks=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b4615bd-faf4-459b-bbf6-1782675bcbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 job postings in HTML.\n",
      "Successfully processed 40 jobs and saved to 'jobs_processed'.\n"
     ]
    }
   ],
   "source": [
    "# Extract job info from raw HTML and save structured data\n",
    "scraper.process_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54e149db-23a9-4ad6-b440-0b883e1a6c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Must-have skills for 40 jobs\n",
      "‚úî Technical Business Analyst with German ‚Üí ['business analyst', 'html', 'css', 'javascript', 'frontend', 'banking services', 'financial services']\n",
      "‚úî Guidewire Business Analyst ‚Üí ['guidewire', 'gosu', 'bpmn', 'jira', 'confluence', 'enterprise architect', 'business analysis']\n",
      "‚úî Business Analyst ‚Üí ['business analyst', 'cybersecurity', 'stakeholder management', 'risk management', 'business analysis', 'jira', 'confluence', 'visio', 'data analysis', 'excel', 'bi', 'project management']\n",
      "‚úî Business Analyst ‚Üí ['jira']\n",
      "‚úî Business Analyst ‚Üí ['business analyst', 'customer-facing products', 'mobile application', 'web application', 'agile', 'communication skills', 'analytical skills', 'uml', 'bpmn']\n",
      "‚úî Business Analyst ‚Üí ['analytical skills', 'stakeholder management', 'data visualisation & presentation']\n",
      "‚úî Business-System Analyst ‚Üí ['sql', 'uml', 'bpmn', 'enterprise architect']\n",
      "‚úî Junior Operations Business Analyst ‚Üí []\n",
      "‚úî Business Analyst ‚Üí ['ux', 'figma', 'cx', 'jira', 'confluence']\n",
      "‚úî Global FIS Application Owner ‚Äì IT Business Analyst ‚Üí ['application owner', 'business analyst', 'banking apis', 'swift', 'erp integration', 'm3', 'iscala', 'oracle', 'fis configuring', 'integrity - level 1 certification']\n",
      "‚úî Senior Business Analyst ‚Üí ['testing', 'sql', 'analytical skills', 'business analysis', 'data analysis', 'excel']\n",
      "‚úî Senior Business Analyst ‚Üí ['data governance', 'data management', 'agile']\n",
      "‚úî Senior Business System Analyst ‚Üí ['business analysis', 'system analysis', 'uml', 'bpmn']\n",
      "‚úî Senior Business Analyst ‚Üí ['salesforce']\n",
      "‚úî IT Business Analyst | System Analyst | Mid | Senior ‚Üí ['business analysis', 'system analysis']\n",
      "‚úî Business Analyst Financial Services ‚Üí ['business analyst']\n",
      "‚úî Business Analyst (Guidewire) ‚Üí ['guidewire', 'bpmn', 'uml', 'kanban', 'stakeholder management']\n",
      "‚úî Insurance Data & Business Analyst ‚Üí ['relational database', 'data warehouses', 'data visualization', 'sql', 'degree']\n",
      "‚úî Business Analyst (PAM) ‚Üí ['pam', 'jira', 'excel', 'visio', 'business analysis']\n",
      "‚úî Strategy and Investment Analyst - new business ‚Üí ['excel', 'vba', 'power bi', 'sql', 'degree', 'stakeholder management', 'communication skills']\n",
      "‚úî Product Owner / Business Analyst ‚Äì Credit Risk ‚Üí ['business analyst', 'user stories', 'excel', 'sql', 'bdd', 'testing', 'jira', 'confluence', 'bpmn', 'business analysis', 'uml', 'credit risk']\n",
      "‚úî Finance Systems Business Analyst ‚Üí ['business analyst', 'accounting', 'erp', 'testing', 'uat', 'communication skills']\n",
      "‚úî Senior Business Analyst ‚Üí ['bpmn', 'uml', 'sql']\n",
      "‚úî Technical Business Analyst (Cybersecurity) ‚Üí ['business analyst', 'iam', 'api', 'communication skills', 'active directory', 'unix']\n",
      "‚úî Business - System Analyst ‚Üí ['enterprise architect', 'bpmn', 'uml', 'splunk', 'rest api', 'kafka', 'sql']\n",
      "‚úî Business Analyst ‚Üí ['business analysis', 'sql', 'agile']\n",
      "‚úî Cloud Business Analyst ‚Üí ['excel', 'vba', 'api', 'google sheets', 'powerbi', 'r']\n",
      "‚úî Murex Specialist / Business Analyst ‚Üí ['murex', 'analytical skills']\n",
      "‚úî (Cybersecurity) Business Analyst ‚Üí ['business analyst', 'cybersecurity', 'cloud', 'security', 'analytical skills', 'business analysis', 'jira', 'confluence', 'visio', 'excel', 'power bi']\n",
      "‚úî Business Analyst ‚Üí ['jira', 'confluence']\n",
      "‚úî Business & Funds Reconciliation Analyst ‚Üí ['degree', 'ms office suite']\n",
      "‚úî Senior Business Analyst ‚Üí ['business analysis', 'e-commerce', 'retail', 'logistics', 'ux', 'kpi', 'jira', 'confluence', 'excel', 'power bi', 'uml', 'bpmn', 'rest', 'soap', 'sql']\n",
      "‚úî Business Analyst ‚Üí ['analytical skills', 'communication abilities']\n",
      "‚úî Salesforce Business Analyst ‚Üí ['salesforce', 'business analyst', 'uat']\n",
      "‚úî System - Business Analyst ‚Üí ['uml', 'bpmn', 'jira', 'confluence', 'sql', 'kafka', 'api']\n",
      "‚úî Senior Business Analyst ‚Üí []\n",
      "‚úî Cloud Security Business Analyst ‚Üí ['business analyst', 'security', 'saas', 'cybersecurity', 'user stories', 'api', 'microservices', 'kubernetes', 'cloud platform', 'gcp', 'devops', 'stakeholder management', 'communication skills', 'cloud']\n",
      "‚úî Business Analyst ‚Üí ['business analysis', 'requirements gathering', 'project management', 'confluence', 'jira', 'ms office']\n",
      "‚úî Senior Business Analyst ‚Üí ['use cases', 'user stories', 'safe', 'communication skills', 'ms office', 'visio', 'powerpoint', 'jira', 'confluence', 'stakeholder management']\n",
      "‚úî Business-System Analyst ‚Üí ['uml', 'bpmn', 'jira', 'confluence', 'enterprise architect', 'knf', 'gdpr']\n",
      "Finished scraping Must-have skills.\n"
     ]
    }
   ],
   "source": [
    "# Scrape Must-have skills from each job page\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_must_have_skills(db, limit=100):\n",
    "    driver = webdriver.Chrome()\n",
    "    wait = WebDriverWait(driver, 15)\n",
    "\n",
    "    jobs = list(db.jobs_processed.find().limit(limit))\n",
    "    print(f\"Scraping Must-have skills for {len(jobs)} jobs\")\n",
    "\n",
    "    for job in jobs:\n",
    "        url = job.get(\"jump_url\")\n",
    "        if not url:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(2)  # wait for page to render\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            must_section = soup.select_one('section[branch=\"musts\"]')\n",
    "\n",
    "            skills = []\n",
    "            if must_section:\n",
    "                skill_tags = must_section.select('span[id^=\"item-tag-\"]')\n",
    "                for tag in skill_tags:\n",
    "                    skills.append(tag.get_text(strip=True).lower())\n",
    "\n",
    "            # Update MongoDB document\n",
    "            db.jobs_processed.update_one(\n",
    "                {\"_id\": job[\"_id\"]},\n",
    "                {\"$set\": {\"must_have_skills\": skills}}\n",
    "            )\n",
    "\n",
    "            print(f\"‚úî {job['job_title']} ‚Üí {skills}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error scraping {url}: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "    print(\"Finished scraping Must-have skills.\")\n",
    "\n",
    "# Run the Must-have skills scraper\n",
    "scrape_must_have_skills(scraper.db, limit=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38649705-70f1-41c7-87c8-539c0f64d7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
